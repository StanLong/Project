# 用户行为数据采集

## Flume拦截器

![](./doc/11.png)

```java
package com.atguigu.flume.interceptor;

import org.apache.flume.Context;
import org.apache.flume.Event;
import org.apache.flume.interceptor.Interceptor;

import java.nio.charset.StandardCharsets;
import java.util.Iterator;
import java.util.List;

/**
 * 过滤非JSON格式的数据
 */
public class ETLInterceptor implements Interceptor {
    @Override
    public void initialize() {

    }

    @Override
    public Event intercept(Event event) {

        byte[] body = event.getBody();
        String log = new String(body, StandardCharsets.UTF_8);

        if (JSONUtils.isJSONValidate(log)) {
            return event;
        } else {
            return null;
        }
    }

    @Override
    public List<Event> intercept(List<Event> list) {

        Iterator<Event> iterator = list.iterator();

        while (iterator.hasNext()){
            Event next = iterator.next();
            if(intercept(next)==null){
                iterator.remove();
            }
        }

        return list;
    }

    public static class Builder implements Interceptor.Builder{

        @Override
        public Interceptor build() {
            return new ETLInterceptor();
        }
        @Override
        public void configure(Context context) {

        }

    }

    @Override
    public void close() {

    }

}
```

```java
package com.atguigu.flume.interceptor;

import com.alibaba.fastjson.JSON;
import com.alibaba.fastjson.JSONException;

/**
 * JSON格式校验工具类
 */
public class JSONUtils {

    public static boolean isJSONValidate(String log){
        try {
            JSON.parse(log);
            return true;
        }catch (JSONException e){
            return false;
        }
    }

}
```

```java
package com.atguigu.flume.interceptor;

import com.alibaba.fastjson.JSONObject;
import org.apache.flume.Context;
import org.apache.flume.Event;
import org.apache.flume.interceptor.Interceptor;

import java.nio.charset.StandardCharsets;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;

/**
 * 时间戳拦截器，解决零点漂移的问题
 */
public class TimeStampInterceptor implements Interceptor{

    private ArrayList<Event> events = new ArrayList<>();

    @Override
    public void initialize() {

    }

    @Override
    public Event intercept(Event event) {

        Map<String, String> headers = event.getHeaders();
        String log = new String(event.getBody(), StandardCharsets.UTF_8);

        JSONObject jsonObject = JSONObject.parseObject(log);

        String ts = jsonObject.getString("ts");
        headers.put("timestamp", ts);

        return event;
    }

    @Override
    public List<Event> intercept(List<Event> list) {
        events.clear();
        for (Event event : list) {
            events.add(intercept(event));
        }

        return events;
    }

    @Override
    public void close() {

    }

    public static class Builder implements Interceptor.Builder {
        @Override
        public Interceptor build() {
            return new TimeStampInterceptor();
        }

        @Override
        public void configure(Context context) {
        }
    }
}
```

pom.xml

```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>org.example</groupId>
    <artifactId>flume-interceptor</artifactId>
    <version>1.0-SNAPSHOT</version>

    <dependencies>
        <dependency>
            <groupId>org.apache.flume</groupId>
            <artifactId>flume-ng-core</artifactId>
            <version>1.9.0</version>
            <scope>provided</scope>
        </dependency>

        <dependency>
            <groupId>com.alibaba</groupId>
            <artifactId>fastjson</artifactId>
            <version>1.2.62</version>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>2.3.2</version>
                <configuration>
                    <source>1.8</source>
                    <target>1.8</target>
                </configuration>
            </plugin>
            <plugin>
                <artifactId>maven-assembly-plugin</artifactId>
                <configuration>
                    <descriptorRefs>
                        <descriptorRef>jar-with-dependencies</descriptorRef>
                    </descriptorRefs>
                </configuration>
                <executions>
                    <execution>
                        <id>make-assembly</id>
                        <phase>package</phase>
                        <goals>
                            <goal>single</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
</project>
```

 编译打包拦截器，生成 flume-interceptor-1.0-SNAPSHOT-jar-with-dependencies.jar 上传到 flume lib目录下

```shell
[root@node01 lib]# pwd
/opt/stanlong/flume/apache-flume-1.9.0-bin/lib
```

并分发到其他节点

## 日志采集Flume

![](./doc/02.png)

日志采集Flume配置文件

```shell
[root@node01 conf]# pwd
/opt/stanlong/flume/apache-flume-1.9.0-bin/conf
[root@node01 conf]# vi file-flume-kafka.conf
```

```properties
# 一、定义组件
a1.sources=r1
a1.channels=c1

# 二、配置 source
a1.sources.r1.type = TAILDIR
a1.sources.r1.positionFile = /opt/stanlong/flume/tmp/taildir_position.json
a1.sources.r1.filegroups = f1
# 监控的文件所在路径，文件名称用正则表达示匹配
a1.sources.r1.filegroups.f1 = /root/warehouse40/applog/log/app.*
a1.sources.r1.fileHeader = true

# 三、配置拦截器
a1.sources.r1.interceptors =  i1
a1.sources.r1.interceptors.i1.type = com.atguigu.flume.interceptor.ETLInterceptor$Builder

# 四、channel
a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c1.kafka.bootstrap.servers = node01:9092,node02:9092,node03:9092
a1.channels.c1.kafka.topic = topic-log
a1.channels.c1.parseAsFlumeEvent = false
a1.channels.c1.kafka.consumer.group.id = flume-consumer

# 五、拼接组件
a1.sources.r1.channels = c1
```

## 日志消费Flume

![](./doc/03.png)

日志采集Flume配置文件

```shell
[root@node01 conf]# pwd
/opt/stanlong/flume/apache-flume-1.9.0-bin/conf
[root@node01 conf]# vi kafka-flume-hdfs.conf
```

```properties
# 定义组件
a1.sources=r1
a1.channels=c1
a1.sinks=k1

# 配置 source
a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.r1.batchSize = 5000
a1.sources.r1.batchDurationMillis = 2000
a1.sources.r1.kafka.bootstrap.servers = node01:9092,node02:9092,node03:9092
a1.sources.r1.kafka.topics=topic-log

# 配置拦截器
a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type = com.atguigu.flume.interceptor.TimeStampInterceptor$Builder

#  配置 channel
a1.channels.c1.type = file
a1.channels.c1.checkpointDir = /opt/stanlong/flume/apache-flume-1.9.0-bin/checkpoint/behavior1
a1.channels.c1.dataDirs = /opt/stanlong/flume/apache-flume-1.9.0-bin/data/behavior1/


# 配置 sink
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = hdfs://hacluster/origin_data/gmall/log/topic-log/%Y-%m-%d
a1.sinks.k1.hdfs.filePrefix = log-
a1.sinks.k1.hdfs.round = false

# 控制生成的小文件
a1.sinks.k1.hdfs.rollInterval = 10
a1.sinks.k1.hdfs.rollSize = 134217728
a1.sinks.k1.hdfs.rollCount = 0

## 控制输出文件是原生文件。
a1.sinks.k1.hdfs.fileType = CompressedStream
a1.sinks.k1.hdfs.codeC = lzop

# 拼接组件
a1.sources.r1.channels = c1
a1.sinks.k1.channel= c1
```

## 测试数据采集

### 新建topic

1. 启动kafka

2. 创建topic

   ```shell
   [root@node01 myshell]# kafka-topics.sh --zookeeper node02:2181 --create --replication-factor 3 --partitions 1 --topic topic-log
   Created topic topic-log.
   
   [root@node01 myshell]# kafka-topics.sh --zookeeper node02:2181 --list
   __consumer_offsets
   sensor
   spark-kafka
   topic-log
   [root@node01 myshell]# 
   ```

### 生成用户访问日志文件

### ![](./doc/12.png)

 日志文件存放路径配置在 logback.xml 

   ```xml
   <?xml version="1.0" encoding="UTF-8"?>
   <configuration>
       <property name="LOG_HOME" value="/opt/stanlong/applog/log" />
       <appender name="console" class="ch.qos.logback.core.ConsoleAppender">
           <encoder>
               <pattern>%msg%n</pattern>
           </encoder>
       </appender>
   
       <appender name="rollingFile" class="ch.qos.logback.core.rolling.RollingFileAppender">
           <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
               <fileNamePattern>${LOG_HOME}/app.%d{yyyy-MM-dd}.log</fileNamePattern>
           </rollingPolicy>
           <encoder>
               <pattern>%msg%n</pattern>
           </encoder>
       </appender>
   
       <!-- 将某一个包下日志单独打印日志 -->
       <logger name="com.atgugu.gmall2020.mock.log.util.LogUtil"
               level="INFO" additivity="false">
           <appender-ref ref="rollingFile" />
           <appender-ref ref="console" />
       </logger>
   
       <root level="error"  >
           <appender-ref ref="console" />
       </root>
   </configuration>
   
   ```

 日志生成时间配置在 application.properties

   ```properties
   logging.config=./logback.xml
    mock.date=2021-06-01
    mock.type=log
    mock.startup.count=200
    mock.max.mid=500000
    mock.max.uid=500
    mock.max.sku-id=35
    mock.page.during-time-ms=20000
    mock.error.rate=3
    mock.log.sleep=10
    mock.detail.source-type-rate=40:25:15:20
    mock.if_get_coupon_rate=75
    mock.max.coupon-id=3
    mock.search.keyword=图书,小米,iphone11,电视,口红,ps5,苹果手机,小米盒子
   ```

 linux 时间戳转日期格式

   ```shell
date -d @1556519685 +"%Y-%m-%d %H:%M:%S"
   ```

 生成 2021-06-01 到 2021-06-07 的数据

### 采集日志

**flume 采集日志脚本**

```shell
[root@node01 appmain]# vi flume-producer.sh 
# flume 日志目录要提前建好，不然脚本执行没有反应
LOG_HOME="/var/data/flume/log"
case $1 in
"start"){
    for i in node01
    do
        echo " --------启动 $i 采集flume-------"
        ssh $i "nohup flume-ng agent --conf-file /opt/stanlong/flume/jobs/file-flume-kafka.conf --name a1 -Dflume.root.logger=INFO,LOGFILE >$LOG_HOME/log1.txt 2>&1 &"
    done
};;
"stop"){
        for i in node01
        do
                echo " --------停止 $i 采集flume-------"
                ssh $i "ps -ef | grep file-flume-kafka | grep -v grep |awk  '{print \$2}' | xargs -n1 kill -9 "
        done

};;
esac

[root@node01 appmain]# chmod 755 flume-producer.sh 
```

启动kafka消费者

```shell
[root@node03 ~]# kafka-console-consumer.sh --bootstrap-server node02:9092 --from-beginning --topic topic-log
```

等个几秒钟，kafka消费者会在控制台打印出消费到的数据。

**flume消费者脚本**

```shell
[root@node01 appmain]# vi flume-consumer.sh

#! /bin/bash

LOG_HOME="/var/data/flume/log"
case $1 in
"start"){
    for i in node03 node04
    do
        echo " --------启动 $i 消费flume-------"
        ssh $i "nohup flume-ng agent --conf-file /opt/stanlong/flume/jobs/kafka-flume-hdfs.conf --name a1 -Dflume.root.logger=INFO,LOGFILE >$LOG_HOME/log2.txt 2>&1 &"
        done
};;
"stop"){
    for i in node03 node04
    do
        echo " --------停止 $i 消费flume-------"
        ssh $i "ps -ef | grep kafka-flume-hdfs | grep -v grep |awk '{print \$2}' | xargs -n1 kill"
    done

};;
esac

[root@node01 appmain]# chmod 755 flume-consumer.sh
```

## 脚本启动顺序

kafka -》 flume消费者 -》 flume 生产者

![](./doc/13.png)

## 报错处理

启动kafka消费者时，有如下报错

```shell
[2022-08-05 17:01:27,565] WARN [Consumer clientId=consumer-console-consumer-49207-1, groupId=console-consumer-49207] Connection to node -1 (node02/192.168.235.12:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
[2022-08-05 17:01:27,565] WARN [Consumer clientId=consumer-console-consumer-49207-1, groupId=console-consumer-49207] Bootstrap broker node02:9092 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient)
[2022-08-05 17:01:29,327] WARN [Consumer clientId=consumer-console-consumer-49207-1, groupId=console-consumer-49207] Connection to node -1 (node02/192.168.235.12:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
[2022-08-05 17:01:29,327] WARN [Consumer clientId=consumer-console-consumer-49207-1, groupId=console-consumer-49207] Bootstrap broker node02:9092 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient)
```

临时处理方法：重建topic.

```shell
# 删除topic
[root@node03 config]# kafka-topics.sh --zookeeper node02:2181 --delete --topic topic-log
Topic topic-log is marked for deletion.
Note: This will have no impact if delete.topic.enable is not set to true.

# 发现topic只是标记删除，并没有真正的删掉
[root@node03 config]# kafka-topics.sh --zookeeper node02:2181 --list
__consumer_offsets
first-topic
topic-log - marked for deletion
[root@node03 config]# 

# 登录zk, 到zk上去删
[root@node03 config]# zkCli.sh
[zk: localhost:2181(CONNECTED) 4] ls /brokers/topics 
[first-topic, topic-log, __consumer_offsets]

# 第一步删除
[zk: localhost:2181(CONNECTED) 5] rmr /brokers/topics/topic-log

# 第二步删除
[zk: localhost:2181(CONNECTED) 7] rmr  /admin/delete_topics/topic-log

# 第三步删除
[zk: localhost:2181(CONNECTED) 1] rmr /config/topics/topic-log

# 重启kafka

# 创建topic
kafka-topics.sh --zookeeper node02:2181 --create --replication-factor 3 --partitions 1 --topic topic-log
```



















