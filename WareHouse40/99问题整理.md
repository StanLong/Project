ODS-业务表

```sql
DROP TABLE IF EXISTS ods_activity_info;
DROP TABLE IF EXISTS ods_activity_rule;
DROP TABLE IF EXISTS ods_base_category1;
DROP TABLE IF EXISTS ods_base_category2;
DROP TABLE IF EXISTS ods_base_category3;
DROP TABLE IF EXISTS ods_base_dic;
DROP TABLE IF EXISTS ods_base_province;
DROP TABLE IF EXISTS ods_base_region;
DROP TABLE IF EXISTS ods_base_trademark;
DROP TABLE IF EXISTS ods_cart_info;
DROP TABLE IF EXISTS ods_comment_info;
DROP TABLE IF EXISTS ods_coupon_info;
DROP TABLE IF EXISTS ods_coupon_use;
DROP TABLE IF EXISTS ods_favor_info;
DROP TABLE IF EXISTS ods_order_detail;
DROP TABLE IF EXISTS ods_order_detail_activity;
DROP TABLE IF EXISTS ods_order_detail_coupon;
DROP TABLE IF EXISTS ods_order_info;
DROP TABLE IF EXISTS ods_order_refund_info;
DROP TABLE IF EXISTS ods_order_status_log;
DROP TABLE IF EXISTS ods_payment_info;
DROP TABLE IF EXISTS ods_refund_payment;
DROP TABLE IF EXISTS ods_sku_attr_value;
DROP TABLE IF EXISTS ods_sku_info;
DROP TABLE IF EXISTS ods_sku_sale_attr_value;
DROP TABLE IF EXISTS ods_spu_info;
DROP TABLE IF EXISTS ods_user_info;
```

DWD层用户行为日志表

```sql
show partitions dwd_start_log;
show partitions dwd_page_log;
show partitions dwd_action_log;
show partitions dwd_display_log;
show partitions dwd_error_log;
```



job not found

```
22/07/31 12:35:08 ERROR tool.ImportTool: Import failed: java.io.IOException: java.io.IOException: Unknown Job job_1659240084844_0008
	at org.apache.hadoop.mapreduce.v2.hs.HistoryClientService$HSClientProtocolHandler.verifyAndGetJob(HistoryClientService.java:220)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryClientService$HSClientProtocolHandler.getTaskAttemptCompletionEvents(HistoryClientService.java:283)
	at org.apache.hadoop.mapreduce.v2.api.impl.pb.service.MRClientProtocolPBServiceImpl.getTaskAttemptCompletionEvents(MRClientProtocolPBServiceImpl.java:173)
	at org.apache.hadoop.yarn.proto.MRClientProtocol$MRClientProtocolService$2.callBlockingMethod(MRClientProtocol.java:283)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1893)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)

	at org.apache.hadoop.mapred.ClientServiceDelegate.invoke(ClientServiceDelegate.java:344)
	at org.apache.hadoop.mapred.ClientServiceDelegate.getTaskCompletionEvents(ClientServiceDelegate.java:396)
	at org.apache.hadoop.mapred.YARNRunner.getTaskCompletionEvents(YARNRunner.java:811)
	at org.apache.hadoop.mapreduce.Job$6.run(Job.java:732)
	at org.apache.hadoop.mapreduce.Job$6.run(Job.java:729)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1893)
	at org.apache.hadoop.mapreduce.Job.getTaskCompletionEvents(Job.java:729)
	at org.apache.hadoop.mapreduce.Job.monitorAndPrintJob(Job.java:1651)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1591)
	at org.apache.sqoop.mapreduce.ImportJobBase.doSubmitJob(ImportJobBase.java:200)
	at org.apache.sqoop.mapreduce.ImportJobBase.runJob(ImportJobBase.java:173)
	at org.apache.sqoop.mapreduce.ImportJobBase.runImport(ImportJobBase.java:270)
	at org.apache.sqoop.manager.SqlManager.importQuery(SqlManager.java:748)
	at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:522)
	at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:628)
	at org.apache.sqoop.Sqoop.run(Sqoop.java:147)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234)
	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243)
	at org.apache.sqoop.Sqoop.main(Sqoop.java:252)
Caused by: java.io.IOException: Unknown Job job_1659240084844_0008
	at org.apache.hadoop.mapreduce.v2.hs.HistoryClientService$HSClientProtocolHandler.verifyAndGetJob(HistoryClientService.java:220)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryClientService$HSClientProtocolHandler.getTaskAttemptCompletionEvents(HistoryClientService.java:283)
	at org.apache.hadoop.mapreduce.v2.api.impl.pb.service.MRClientProtocolPBServiceImpl.getTaskAttemptCompletionEvents(MRClientProtocolPBServiceImpl.java:173)
	at org.apache.hadoop.yarn.proto.MRClientProtocol$MRClientProtocolService$2.callBlockingMethod(MRClientProtocol.java:283)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1893)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:110)
	at org.apache.hadoop.mapreduce.v2.api.impl.pb.client.MRClientProtocolPBClientImpl.unwrapAndThrowException(MRClientProtocolPBClientImpl.java:291)
	at org.apache.hadoop.mapreduce.v2.api.impl.pb.client.MRClientProtocolPBClientImpl.getTaskAttemptCompletionEvents(MRClientProtocolPBClientImpl.java:179)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.mapred.ClientServiceDelegate.invoke(ClientServiceDelegate.java:325)
	... 22 more
Caused by: org.apache.hadoop.ipc.RemoteException(java.io.IOException): Unknown Job job_1659240084844_0008
	at org.apache.hadoop.mapreduce.v2.hs.HistoryClientService$HSClientProtocolHandler.verifyAndGetJob(HistoryClientService.java:220)
	at org.apache.hadoop.mapreduce.v2.hs.HistoryClientService$HSClientProtocolHandler.getTaskAttemptCompletionEvents(HistoryClientService.java:283)
	at org.apache.hadoop.mapreduce.v2.api.impl.pb.service.MRClientProtocolPBServiceImpl.getTaskAttemptCompletionEvents(MRClientProtocolPBServiceImpl.java:173)
	at org.apache.hadoop.yarn.proto.MRClientProtocol$MRClientProtocolService$2.callBlockingMethod(MRClientProtocol.java:283)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1893)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2606)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1507)
	at org.apache.hadoop.ipc.Client.call(Client.java:1453)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy18.getTaskAttemptCompletionEvents(Unknown Source)
	at org.apache.hadoop.mapreduce.v2.api.impl.pb.client.MRClientProtocolPBClientImpl.getTaskAttemptCompletionEvents(MRClientProtocolPBClientImpl.java:177)
	... 27 more
```

又一个报错

```
22/07/31 15:11:46 WARN hdfs.DataStreamer: Exception for BP-967371762-192.168.235.11-1658568655588:blk_1073744854_4035
java.net.SocketTimeoutException: 70000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/192.168.235.13:55670 remote=/192.168.235.13:50010]
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:118)
	at java.io.FilterInputStream.read(FilterInputStream.java:83)
	at java.io.FilterInputStream.read(FilterInputStream.java:83)
	at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:407)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:213)
	at org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1080)
22/07/31 15:12:19 WARN hdfs.DataStreamer: Error Recovery for BP-967371762-192.168.235.11-1658568655588:blk_1073744854_4035 in pipeline [DatanodeInfoWithStorage[192.168.235.13:50010,DS-6e7b6eee-8160-48b8-8ca3-f0cb8007ce38,DISK], DatanodeInfoWithStorage[192.168.235.11:50010,DS-edfcbc3e-74c1-4a94-8201-343fd8881517,DISK]]: datanode 0(DatanodeInfoWithStorage[192.168.235.13:50010,DS-6e7b6eee-8160-48b8-8ca3-f0cb8007ce38,DISK]) is bad.
22/07/31 15:12:55 WARN hdfs.DataStreamer: Slow waitForAckedSeqno took 138012ms (threshold=30000ms). File being written: /tmp/hadoop-yarn/staging/root/.staging/job_1659246358304_0001/libjars/avro-1.8.1.jar, block: BP-967371762-192.168.235.11-1658568655588:blk_1073744854_4036, Write pipeline datanodes: null.
Sun Jul 31 15:14:06 CST 2022 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.

```

lzo索引报错

// 报错的关键点在 When importing query results in parallel, you must specify --split-by， 当 --num-mappers 大于1时，必须配置  --split-by

```
[root@node03 myshell]# ./mysql_to_hdfs_full.sh all 2022-06-13
Warning: /opt/stanlong/sqoop/sqoop/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /opt/stanlong/sqoop/sqoop/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /opt/stanlong/sqoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
22/08/01 21:53:42 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
22/08/01 21:53:42 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
When importing query results in parallel, you must specify --split-by.
Try --help for usage instructions.
22/08/01 21:53:43 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library from the embedded binaries
22/08/01 21:53:43 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 5dbdddb8cfb544e58b4e0b9664b9d1b66657faf5]
22/08/01 21:53:54 WARN lzo.DistributedLzoIndexer: Error walking path: /origin_data/gmall/db/base_category1/2022-06-13
java.io.FileNotFoundException: File does not exist: /origin_data/gmall/db/base_category1/2022-06-13
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1533)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1526)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1526)
	at com.hadoop.compression.lzo.DistributedLzoIndexer.walkPath(DistributedLzoIndexer.java:50)
	at com.hadoop.compression.lzo.DistributedLzoIndexer.run(DistributedLzoIndexer.java:103)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at com.hadoop.compression.lzo.DistributedLzoIndexer.main(DistributedLzoIndexer.java:159)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:244)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:158)
22/08/01 21:53:54 INFO lzo.DistributedLzoIndexer: No input paths found - perhaps all .lzo files have already been indexed.
Warning: /opt/stanlong/sqoop/sqoop/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /opt/stanlong/sqoop/sqoop/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /opt/stanlong/sqoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
22/08/01 21:53:56 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
22/08/01 21:53:57 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
When importing query results in parallel, you must specify --split-by.
Try --help for usage instructions.
22/08/01 21:53:58 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library from the embedded binaries
22/08/01 21:53:58 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 5dbdddb8cfb544e58b4e0b9664b9d1b66657faf5]
22/08/01 21:54:07 WARN lzo.DistributedLzoIndexer: Error walking path: /origin_data/gmall/db/base_category2/2022-06-13
java.io.FileNotFoundException: File does not exist: /origin_data/gmall/db/base_category2/2022-06-13
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1533)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1526)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1526)
	at com.hadoop.compression.lzo.DistributedLzoIndexer.walkPath(DistributedLzoIndexer.java:50)
	at com.hadoop.compression.lzo.DistributedLzoIndexer.run(DistributedLzoIndexer.java:103)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at com.hadoop.compression.lzo.DistributedLzoIndexer.main(DistributedLzoIndexer.java:159)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:244)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:158)
22/08/01 21:54:07 INFO lzo.DistributedLzoIndexer: No input paths found - perhaps all .lzo files have already been indexed.
Warning: /opt/stanlong/sqoop/sqoop/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /opt/stanlong/sqoop/sqoop/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /opt/stanlong/sqoop/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
22/08/01 21:54:09 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
22/08/01 21:54:09 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
When importing query results in parallel, you must specify --split-by.
Try --help for usage instructions.
22/08/01 21:54:10 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library from the embedded binaries
22/08/01 21:54:10 INFO lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 5dbdddb8cfb544e58b4e0b9664b9d1b66657faf5]
22/08/01 21:54:12 WARN lzo.DistributedLzoIndexer: Error walking path: /origin_data/gmall/db/base_category3/2022-06-13
java.io.FileNotFoundException: File does not exist: /origin_data/gmall/db/base_category3/2022-06-13
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1533)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1526)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1526)
	at com.hadoop.compression.lzo.DistributedLzoIndexer.walkPath(DistributedLzoIndexer.java:50)
	at com.hadoop.compression.lzo.DistributedLzoIndexer.run(DistributedLzoIndexer.java:103)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
	at com.hadoop.compression.lzo.DistributedLzoIndexer.main(DistributedLzoIndexer.java:159)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:244)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:158)
```

