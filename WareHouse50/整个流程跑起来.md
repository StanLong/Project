# 整个流程跑起来

# 一、数据模拟

## 1、模拟日志数据

模拟数据程序

- application.yml  
- gmall2020-mock-log-2021-10-10.jar  
- logback.xml 
- path.json

执行程序生成模拟日志数据

```shell
java -jar gmall2020-mock-log-2021-10-10.jar  
-- 执行成功后会在当前目录生成一个 logs 目录
```

## 2、模拟业务数据

模拟数据程序

- application.properties
- gmall2020-mock-db-2021-11-14.jar
- gmall.sql

执行程序生成模拟业务数据

```shell
java -jar gmall2020-mock-db-2021-11-14.jar 

-- 如果是第一次执行， application.properties 文件里，如下两个字段要设置成1
mock.clear=1
mock.clear.user=1
```

# 二、数据采集

## 1、采集业务数据

根据数据流向图，业务数据采集和两个流向

流程一：mysql -》增量同步 Maxwell -》 消息缓存Kafka集群 -》业务消费Flume -》集群存储Hadoop

流向二： mysql-》 每日全量 datax -》集群存储hadoop

### （1）、流向一

1、启动如下脚本

- ha.sh start
- kafka.sh start
- maxwell.sh start
- flume_consumer_db.sh start

执行 `java -jar gmall2020-mock-db-2021-11-14.jar ` 生成业务数据

执行 `kafka-console-consumer.sh --bootstrap-server node02:9092 --topic topic_db` 观察消费者输出

观察Hdfs上  ` /origin_data/gmall/db` 是否有 `_inc` 结尾的目录

2、首日全量同步

启动如下脚本

- ha.sh start
- kafka.sh start
- maxwell.sh start
- flume_consumer_db.sh start

执行 `mysql_maxwell_kafka_flume_full_init.sh` 观察hdfs上是否生成 `/origin_data` 目录， 这个脚本只执行一次即可。

### （2）、流向二

启动如下脚本

- ha.sh start

执行 ` bash gen_import_config.sh` 生成表对应的datax需要的json文件

执行 `bash mysql_datax_hdfs_full.sh all`  执行Datax命令，全量同步mysql数据到hdfs

或者执行 `bash mysql_datax_hdfs_full.sh all 2025-05-10` 指定同步日期

执行 `java -jar gmall2020-mock-db-2021-11-14.jar ` 生成业务数据

执行 `kafka-console-consumer.sh --bootstrap-server node02:9092 --topic topic_db` 观察消费者输出

观察hdfs路径 `/origin_data/gmall/db` 看是否生成 `_full` 结尾的目录

## 2、采集日志数据

启动如下脚本

- ha.sh start
- kafka.sh start
- flume_producer_log.sh start
- flume_consumer_log.sh start start

执行 `java -jar gmall2020-mock-log-2021-10-10.jar  ` 生成日志数据

执行 `kafka-console-consumer.sh --bootstrap-server node02:9092 --topic topic_log` 观察消费者输出

观察hdfs路径 `/origin_data/gmall/log/topic_log` 看是否生成对应的日期目录







